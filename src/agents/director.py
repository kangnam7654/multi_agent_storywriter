import json

from langchain_core.messages import HumanMessage, SystemMessage, ToolMessage
from langchain_ollama import ChatOllama

from src.agents.tools.search_lorebook import search_lorebook
from src.schemas.state import EvalReport, GraphState


class Director:
    """ìŠ¤í† ë¦¬ ê²€ìˆ˜ ì—ì´ì „íŠ¸ (Director)"""

    def __init__(self, llm: ChatOllama, system_prompt: str = ""):
        self.llm_with_tools = llm.bind_tools([search_lorebook])
        self.system_prompt = system_prompt

    def __call__(self, state: GraphState, runtime) -> GraphState:
        """ìŠ¤í† ë¦¬ ê²€ìˆ˜ ì‹¤í–‰"""
        user_message = self._build_user_message(state)

        # ì—ì´ì „íŠ¸ë³„ ë…ë¦½ì ì¸ ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸ ì‚¬ìš© (context í¬ê¸° ì œí•œ)
        messages = [
            SystemMessage(content=self.system_prompt),
            HumanMessage(content=user_message),
        ]

        ai_message = self.llm_with_tools.invoke(messages)
        messages.append(ai_message)

        print(f"ðŸ” Director 1ì°¨ ì‘ë‹µ content: '{ai_message.content}'")
        print(f"ðŸ” Director 1ì°¨ ì‘ë‹µ tool_calls: {ai_message.tool_calls}")

        if ai_message.tool_calls:
            print(f"ðŸ•µï¸ Directorê°€ ì„¤ì •ì„ ê²€ìƒ‰í•©ë‹ˆë‹¤: {ai_message.tool_calls}")
            for tool_call in ai_message.tool_calls:
                # í•¨ìˆ˜ ì‹¤í–‰ (Lorebook ê²€ìƒ‰)
                if tool_call["name"] == "search_lorebook":
                    tool_result = search_lorebook.invoke(tool_call)
                    # ê²€ìƒ‰ ê²°ê³¼ í¬ê¸° ì œí•œ (ìµœëŒ€ 1000ìž)
                    tool_result_str = str(tool_result)[:1000]
                    print(f"ðŸ“š Lorebook ê²€ìƒ‰ ê²°ê³¼: {tool_result_str[:200]}...")

                    # ê²°ê³¼ë¥¼ ë©”ì‹œì§€ì— ì¶”ê°€ (ToolMessage)
                    messages.append(
                        ToolMessage(
                            content=tool_result_str, tool_call_id=tool_call["id"]
                        )
                    )

            final_response = self.llm_with_tools.invoke(messages)
            response_text = final_response.content
        else:
            # ë„êµ¬ë¥¼ ì•ˆ ì¼ìœ¼ë©´ ë°”ë¡œ ê²°ê³¼ ì‚¬ìš©
            response_text = ai_message.content

        # 5. JSON íŒŒì‹± (response_text íƒ€ìž… í™•ì¸)
        print(f"ðŸ” Director ì›ë³¸ ì‘ë‹µ: '{response_text}'")
        print(
            f"ðŸ” Director ì‘ë‹µ íƒ€ìž…: {type(response_text)}, ê¸¸ì´: {len(response_text) if response_text else 0}"
        )

        if not response_text or (
            isinstance(response_text, str) and not response_text.strip()
        ):
            print("âš ï¸ Director ì‘ë‹µì´ ë¹„ì–´ìžˆìŠµë‹ˆë‹¤.")
            eval_report = EvalReport(
                is_approved=False,
                score=0.0,
                feedback="Director ì‘ë‹µ ì˜¤ë¥˜: ì‘ë‹µì´ ë¹„ì–´ìžˆìŠµë‹ˆë‹¤.",
                issues=["ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜"],
            )
        elif not isinstance(response_text, str):
            print(f"âš ï¸ Director ì‘ë‹µì´ ë¬¸ìžì—´ì´ ì•„ë‹™ë‹ˆë‹¤: {type(response_text)}")
            eval_report = EvalReport(
                is_approved=False,
                score=0.0,
                feedback=f"Director ì‘ë‹µ ì˜¤ë¥˜: ì‘ë‹µ íƒ€ìž…ì´ {type(response_text)}ìž…ë‹ˆë‹¤.",
                issues=["ì‘ë‹µ í˜•ì‹ ì˜¤ë¥˜"],
            )
        else:
            print(f"ðŸ” Director ì‘ë‹µ íŒŒì‹± ì¤‘: {response_text[:200]}...")
            eval_report = self._parse_response(response_text)

        # ê²°ê³¼ ë°˜í™˜ (ê¸°ì¡´ ë¡œì§ ë™ì¼)
        state.eval_report = eval_report
        if not eval_report.is_approved:
            state.feedback_history.append(eval_report.feedback)
            state.retry_count += 1
        if eval_report.is_approved:
            state.is_complete = True
            print("âœ… ìŠ¤í† ë¦¬ê°€ ìŠ¹ì¸ë˜ì—ˆìŠµë‹ˆë‹¤.")
        elif state.retry_count >= state.max_retries:
            state.is_complete = True
            print("âš ï¸ ìµœëŒ€ ìž¬ì‹œë„ íšŸìˆ˜ì— ë„ë‹¬í•˜ì—¬ ì¢…ë£Œí•©ë‹ˆë‹¤.")
        return state

    def _build_user_message(self, state: GraphState) -> str:
        """ìœ ì € ë©”ì‹œì§€ êµ¬ì„±"""
        parts = []

        # ì›ë³¸ ìš”ì²­ ì •ë³´
        if state.request:
            parts.append("## Request")
            if state.request.summarized_prompt:
                parts.append(f"Prompt: {state.request.summarized_prompt}")
            if state.request.genre:
                parts.append(f"Genre: {state.request.genre}")
            if state.request.style:
                parts.append(f"Style: {state.request.style}")
            if state.request.length:
                parts.append(f"Length: {state.request.length}")
            parts.append("")

        # ê²€ìˆ˜í•  ìŠ¤í† ë¦¬
        parts.append("## Story to Review")
        if state.story_output and state.story_output.story:
            parts.append(state.story_output.story)
        else:
            parts.append("(ìŠ¤í† ë¦¬ ì—†ìŒ)")
        parts.append("")

        # ìž¬ì‹œë„ ì •ë³´
        parts.append("## Review Info")
        parts.append(f"Attempt: {state.retry_count} / {state.max_retries}")
        return "\n".join(parts)

    def _parse_response(self, response: str) -> EvalReport:
        """LLM ì‘ë‹µì„ EvalReportë¡œ íŒŒì‹±"""
        try:
            # JSON ë¸”ë¡ ì¶”ì¶œ ì‹œë„
            if "```json" in response:
                json_str = response.split("```json")[1].split("```")[0].strip()
            elif "```" in response:
                json_str = response.split("```")[1].split("```")[0].strip()
            else:
                json_str = response.strip()

            data = json.loads(json_str)
            return EvalReport(
                is_approved=data.get("is_approved", False),
                score=float(data.get("score", 0.0)),
                feedback=data.get("feedback", ""),
                issues=data.get("issues", []),
            )
        except (json.JSONDecodeError, KeyError, IndexError) as e:
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê¸°ë³¸ê°’ ë°˜í™˜ (ë¶ˆí•©ê²© ì²˜ë¦¬)
            return EvalReport(
                is_approved=False,
                score=0.0,
                feedback=f"Failed to parse evaluation response: {str(e)}. Raw response: {response[:500]}",
                issues=["Evaluation parsing failed"],
            )

    def set_system_prompt(self, prompt: str):
        self.system_prompt = prompt

    def get_system_prompt(self) -> str:
        return self.system_prompt
